{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kerastuner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5c4ab2653ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_all_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kerastuner'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "import random\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "\n",
    "#要用time series split因为不能用当天的数据预测当天的stock 要用前几天的\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5  #自己设 做几个folder的cross validation 如果是5就是80%train20%test 10就是90%train20%test？？\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None  #自己设 希望trian和test数据之间有缝隙 eg 120个data 前80个trian 后20个test 隔了20个 为了防止数据缺失因为每个date的trade个数不一样？？\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10f5482d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCVTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kt' is not defined"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "#deep learning在建立每一层model时都可以调整每一层的参数 用Cross validation tuner这个Class找最佳的参数\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),  #通过对比y_test的prediction和y_test来决定跑几轮neural network\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True  #training和inference的唯一区别：TRAINING和USE_FINETUNE\n",
    "USE_FINETUNE = False    #TRIANING和USE_FINETUNE用来控制之后的ifelse循环\n",
    "FOLDS = xx # 自己设 一般5-10 \n",
    "SEED = xx # 自己设 any integer\n",
    "\n",
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "#  Jane Street modify their trading model around day 85\n",
    "# https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930\n",
    "#这个帖子说85天之前数据上升很大 可能janestreet在85天后用了新的计算方式 要drop掉前85天的数据\n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #把float64变成了float32 limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "# missing value的处理：mean和median都试试 can try fillna(train.median())\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "# drop了weight为零的 no contribution to the scoring evaludation\n",
    "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
    "#设置action何时为0何时为1\n",
    "# you can adjust this strategies based on the prediction result 这里定义的是5个resp都大于0时action才是1\n",
    "# train['action']= (train['resp'] > 0).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values #feature们\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #从5个resp数值看是否为0还是1 有2^5种可能 #Multitarget classfication\n",
    "# input: features, output: whether resp is bigger than 0\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理noise outliers用下面网站这个function output比input减少了outlier\n",
    "#Creating the autoencoder.¶\n",
    "#The autoencoder should aid in denoising the data:\n",
    "# https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i) #该function是用的neural network的方式所以对起始weight数据非常敏感\n",
    "    #为了降低这种敏感 要一个batch一个batch的放数据 每个batch都要normalize变成mean=0 std=1\n",
    "    # https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "#     Training deep neural networks with tens of layers is challenging as they can be sensitive to \n",
    "#     the initial random weights and configuration of the learning algorithm.\n",
    "#     One possible reason for this difficulty is the distribution of the inputs to layers deep\n",
    "#     in the network may change after each mini-batch when the weights are updated. \n",
    "#     This can cause the learning algorithm to forever chase a moving target. \n",
    "#     This change in the distribution of inputs to layers in the network \n",
    "#     is referred to the technical name “internal covariate shift.”\n",
    "#     Batch normalization is a technique for training very deep neural networks \n",
    "#     that standardizes the inputs to a layer for each mini-batch.\n",
    "#     This has the effect of stabilizing the learning process \n",
    "#     and dramatically reducing the number of training epochs required to train deep networks\n",
    "    encoded = GaussianNoise(noise)(encoded) #加入非零数据减少overfitting\n",
    "    #https://keras.io/api/layers/regularization_layers/gaussian_noise/\n",
    "\n",
    "    #Apply additive zero-centered Gaussian noise.\n",
    "    #This is useful to mitigate overfitting (you could see it as a form of random data augmentation).\n",
    "    #Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.\n",
    "    #stddev: Float, standard deviation of the noise distribution. 0.05\n",
    "    # we can adjust this later 自己设 noise的参数\n",
    "    encoded = Dense(64,activation='relu')(encoded) #flow connected neural network layer见之前cnn的课\n",
    "    #64个headnotes 加了一个activation layer relu=出来的值都大于等于0\n",
    "    # https://keras.io/api/layers/core_layers/dense/\n",
    "    # units: Positive integer, dimensionality of the output space. 64\n",
    "    # activation: Activation function to use.\n",
    "    decoded = Dropout(0.2)(encoded) #加入dropout layer控制overfitting dropout rate是0.2控制多少input unit会drop掉l\n",
    "    #https://keras.io/api/layers/regularization_layers/dropout/\n",
    "    #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, \n",
    "    # which helps prevent overfitting\n",
    "    #rate: Float between 0 and 1. Fraction of the input units to drop. 0.2\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded) #一个dense layer 129个headnotes\n",
    "    x = Dense(32,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=decoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    #https://keras.io/api/models/model_training_apis/\n",
    "    # optimizer: adam\n",
    "    # learning rate: 0.001\n",
    "    # loss function: mse\n",
    "    # minimize crossentropy\n",
    "    return autoencoder, encoder\n",
    "\n",
    "#以上只是对x的预处理 不涉及y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features 作为真正input\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    # 自己设 dropout rate： 0-0.5\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,3)): # num layer：1，2，3 自己设一共跑多少层layer 现在是1，2，3\n",
    "        x = Dense(hp.Int(f'num_units_{i}',64,256))(x)\n",
    "        # 自己设dense layer多少headnots， dense layer unit： 64-256\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "        # 自己设 drop out rate： 0-0.5\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    # 自己设learning rate：0.00001-0.1\n",
    "    # 自己设 label smoothing：0-0.1 让更多的结果是1 make the trade\n",
    "#https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n",
    "#Well, say you were training a model for binary classification. \n",
    "#Your labels would be 0 — cat, 1 — not cat.\n",
    "#Now, say you label_smoothing = 0.2\n",
    "#Using the equation, we get:\n",
    "#new_onehot_labels = [0 1] * (1 — 0.2) + 0.2 / 2 =[0 1]*(0.8) + 0.1\n",
    "#new_onehot_labels =[0.9 0.1]\n",
    "#These are soft labels, instead of hard labels, that is 0 and 1. This will ultimately give you lower loss \n",
    "#when there is an incorrect prediction,and subsequently, your model will penalize and learn incorrectly by a slightly lesser degree.\n",
    "#In essence, label smoothing will help your model to train around mislabeled data and consequently improve its robustness and performance.\n",
    "    # maximize auc\n",
    "    return model\n",
    "# you can adjust the range for tuning parameters by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and training the autoencoder.\n",
    "#We add gaussian noise with mean and std from training data. \n",
    "#After training we lock the layers in the encoder from further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "set_all_seeds(SEED)    \n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1000,\n",
    "                    batch_size=xx, # 2的指数： 1024， 2048，4096\n",
    "    #https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "    #one epoch = one forward pass and one backward pass of all the training examples\n",
    "    #batch size = the number of training examples in one forward/backward pass. \n",
    "    #The higher the batch size, the more memory space you'll need.\n",
    "    #The batch size defines the number of samples that will be propagated through the network.\n",
    "    #For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100.\n",
    "    #The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network.\n",
    "    #Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \n",
    "   #We can keep doing this procedure until we have propagated all samples through of the network. \n",
    "  #Problem might happen with the last set of samples. \n",
    "    #In our example, we've used 1050 which is not divisible by 100 without remainder. \n",
    "     #The simplest solution is just to get the final 50 samples and train the network.\n",
    "    #Advantages of using a batch size < number of all samples:\n",
    "\n",
    "#It requires less memory. Since you train the network using fewer samples,\n",
    "#the overall training procedure requires less memory. \n",
    "#That's especially important if you are not able to fit the whole dataset in your machine's memory.\n",
    "\n",
    "#Typically networks train faster with mini-batches.\n",
    "#That's because we update the weights after each propagation. \n",
    "#In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) \n",
    "#and after each of them we've updated our network's parameters. \n",
    "#If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "#Disadvantages of using a batch size < number of all samples:\n",
    "\n",
    "#The smaller the batch the less accurate the estimate of the gradient will be.\n",
    "                    validation_split=0.1, # training: 90%, validation :10%\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "    # 当validation的loss达到最小，model停止\n",
    "    # patience\tNumber of epochs with no improvement after which training will be stopped. 10\n",
    "    encoder.save_weights('./encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights('../input/自己命名一个folder之后inference会用到/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series cross validaton\n",
    "# https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "#We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP.\n",
    "#We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_auc', direction='max'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=20))\n",
    "# tuning parameters: initial experiment 4, max experiment 20\n",
    "# max cv auc\n",
    "\n",
    "FOLDS = xx # same as before\n",
    "SEED = xx # any interger\n",
    "\n",
    "if TRAINING:\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    # you can try differenr group gap values: 10-50\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=xx,epochs=100,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    # you can try different values for batch size \n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=xx,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        # same values as tuner\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n",
    "        model.fit(X_test,y_test,epochs=3,batch_size=xx)\n",
    "        # same values as tuner\n",
    "        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'../input/自己命名一个folder之后inference会用到/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        if USE_FINETUNE:\n",
    "            model.load_weights(f'../input/自己命名一个folder之后inference会用到/model_{SEED}_{f}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'../input/自己命名一个folder之后inference会用到/model_{SEED}_{f}.hdf5')\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not run for training, will run for prediction\n",
    "if not  TRAINING:\n",
    "    f = np.mean\n",
    "    models = models[-2:]\n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "            pred = f(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
