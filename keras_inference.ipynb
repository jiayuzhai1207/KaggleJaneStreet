{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "import random\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters should have the same values as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "USE_FINETUNE = True \n",
    "FOLDS = xx # 5-10\n",
    "SEED = xx # any integer\n",
    "\n",
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "#  Jane Street modify their trading model around day 85\n",
    "# https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930\n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "# can try fillna(train.median())\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "# no contribution to the scoring evaludation\n",
    "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\n",
    "# you can adjust this strategies based on the prediction result\n",
    "# train['action']= (train['resp'] > 0).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget classfication\n",
    "# input: features, output: whether resp is bigger than 0\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the autoencoder.¶\n",
    "#The autoencoder should aid in denoising the data:\n",
    "# https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    # https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "#     Training deep neural networks with tens of layers is challenging as they can be sensitive to \n",
    "#     the initial random weights and configuration of the learning algorithm.\n",
    "#     One possible reason for this difficulty is the distribution of the inputs to layers deep\n",
    "#     in the network may change after each mini-batch when the weights are updated. \n",
    "#     This can cause the learning algorithm to forever chase a moving target. \n",
    "#     This change in the distribution of inputs to layers in the network \n",
    "#     is referred to the technical name “internal covariate shift.”\n",
    "#     Batch normalization is a technique for training very deep neural networks \n",
    "#     that standardizes the inputs to a layer for each mini-batch.\n",
    "#     This has the effect of stabilizing the learning process \n",
    "#     and dramatically reducing the number of training epochs required to train deep networks\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    #https://keras.io/api/layers/regularization_layers/gaussian_noise/\n",
    "\n",
    "    #Apply additive zero-centered Gaussian noise.\n",
    "    #This is useful to mitigate overfitting (you could see it as a form of random data augmentation).\n",
    "    #Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.\n",
    "    #stddev: Float, standard deviation of the noise distribution. 0.05\n",
    "    # we can adjust this later\n",
    "    encoded = Dense(64,activation='relu')(encoded)\n",
    "    # https://keras.io/api/layers/core_layers/dense/\n",
    "    # units: Positive integer, dimensionality of the output space. 64\n",
    "    # activation: Activation function to use.\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    #https://keras.io/api/layers/regularization_layers/dropout/\n",
    "    #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, \n",
    "    # which helps prevent overfitting\n",
    "    #rate: Float between 0 and 1. Fraction of the input units to drop. 0.2\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(32,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=decoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    #https://keras.io/api/models/model_training_apis/\n",
    "    # optimizer: adam\n",
    "    # learning rate: 0.001\n",
    "    # loss function: mse\n",
    "    # minimize crossentropy\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    # dropout rate： 0-0.5\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,3)): # num layer：1，2，3\n",
    "        x = Dense(hp.Int(f'num_units_{i}',64,256))(x)\n",
    "        # dense layer unit： 64-256， 一般是2的指数， 64， 128， 256\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "        # drop out rate： 0-0.5\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    # learning rate：0.00001-0.1\n",
    "    #label smoothing：0-0.1\n",
    "#https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n",
    "#Well, say you were training a model for binary classification. \n",
    "#Your labels would be 0 — cat, 1 — not cat.\n",
    "#Now, say you label_smoothing = 0.2\n",
    "#Using the equation, we get:\n",
    "#new_onehot_labels = [0 1] * (1 — 0.2) + 0.2 / 2 =[0 1]*(0.8) + 0.1\n",
    "#new_onehot_labels =[0.9 0.1]\n",
    "#These are soft labels, instead of hard labels, that is 0 and 1. This will ultimately give you lower loss \n",
    "#when there is an incorrect prediction,and subsequently, your model will penalize and learn incorrectly by a slightly lesser degree.\n",
    "#In essence, label smoothing will help your model to train around mislabeled data and consequently improve its robustness and performance.\n",
    "    # maximize auc\n",
    "    return model\n",
    "# you can adjust the range for tuning parameters by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and training the autoencoder.\n",
    "#We add gaussian noise with mean and std from training data. \n",
    "#After training we lock the layers in the encoder from further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "set_all_seeds(SEED)    \n",
    "if TRAINING:  #跑train set\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1000, #trianing数据通过neural network train了一次则为一个epoch\n",
    "                    batch_size=xx, #每次放多少数据 全放的话gpu会爆 2的指数： 1024， 2048， 4096\n",
    "    #https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "    #one epoch = one forward pass and one backward pass of all the training examples\n",
    "    #batch size = the number of training examples in one forward/backward pass. \n",
    "    #The higher the batch size, the more memory space you'll need.\n",
    "    #The batch size defines the number of samples that will be propagated through the network.\n",
    "    #For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100.\n",
    "    #The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network.\n",
    "    #Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \n",
    "   #We can keep doing this procedure until we have propagated all samples through of the network. \n",
    "  #Problem might happen with the last set of samples. \n",
    "    #In our example, we've used 1050 which is not divisible by 100 without remainder. \n",
    "     #The simplest solution is just to get the final 50 samples and train the network.\n",
    "    #Advantages of using a batch size < number of all samples:\n",
    "\n",
    "#It requires less memory. Since you train the network using fewer samples,\n",
    "#the overall training procedure requires less memory. \n",
    "#That's especially important if you are not able to fit the whole dataset in your machine's memory.\n",
    "\n",
    "#Typically networks train faster with mini-batches.\n",
    "#That's because we update the weights after each propagation. \n",
    "#In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) \n",
    "#and after each of them we've updated our network's parameters. \n",
    "#If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "#Disadvantages of using a batch size < number of all samples:\n",
    "\n",
    "#The smaller the batch the less accurate the estimate of the gradient will be.\n",
    "                    validation_split=0.1, # training: 90%, validation :10%\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)]) #哪组epoch跑完weight最好 就保存该组\n",
    "    # 当validation的loss达到最小，model停止\n",
    "    # patience\tNumber of epochs with no improvement after which training will be stopped. 10\n",
    "    encoder.save_weights('./encoder.hdf5')\n",
    "else: #跑test set\n",
    "    encoder.load_weights('../input/自己命名一个folder之后inference会用到/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series cross validaton\n",
    "# https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "#We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP.\n",
    "#We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_auc', direction='max'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=20))\n",
    "# tuning parameters: initial experiment 4, max experiment 20\n",
    "# max cv auc\n",
    "\n",
    "FOLDS = xx # same as before\n",
    "SEED = xx # any interger: 3-5\n",
    "\n",
    "if TRAINING:\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    # you can try differenr group gap values: 10-50\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=xx,epochs=100,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    # you can try different values for batch size \n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=xx,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        # same values as tuner\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n",
    "        model.fit(X_test,y_test,epochs=3,batch_size=xx)\n",
    "        # same values as tuner\n",
    "        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'../input/自己命名一个folder之后inference会用到/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        if USE_FINETUNE:\n",
    "            model.load_weights(f'../input/自己命名一个folder之后inference会用到/model_{SEED}_{f}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'../input/自己命名一个folder之后inference会用到/model_{SEED}_{f}.hdf5')\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not run for training, will run for prediction\n",
    "if not  TRAINING:\n",
    "    f = np.mean\n",
    "    models = models[-2:]\n",
    "    # you can adjust the models based on your result, delete the models with lower auc\n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "            pred = f(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
